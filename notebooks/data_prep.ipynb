{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d98a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: raid-bench in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (1.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: scikit-learn~=1.3.2 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from raid-bench) (1.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from scikit-learn~=1.3.2->raid-bench) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from scikit-learn~=1.3.2->raid-bench) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from scikit-learn~=1.3.2->raid-bench) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\zaina\\documents\\ai_detector\\.venv\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas datasets raid-bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b29944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from raid.utils import load_data as load_raid_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5edbc9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAID Loaded: 10000 rows\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('liamdugan/raid', split='train', streaming=True)\n",
    "df_raid = pd.DataFrame(list(ds.take(10000)))\n",
    "\n",
    "# Basic cleaning\n",
    "df_raid = df_raid[['generation', 'model']].rename(columns={'generation': 'text'})\n",
    "df_raid['label'] = df_raid['model'].apply(lambda x: 0 if str(x).lower() == 'human' else 1)\n",
    "\n",
    "df_raid['text'] = df_raid['text'].str.replace(r'[\\u2028\\u2029]', ' ', regex=True)\n",
    "df_raid.to_csv(\"../data/raid_local.csv\", index=False)\n",
    "print(f\"RAID Loaded: {len(df_raid)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a44a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAIGT Loaded: 10000 rows\n"
     ]
    }
   ],
   "source": [
    "df_daigt = pd.read_csv('../data/train_v2_drcat_02.csv').head(10000)[['text', 'label']]\n",
    "df_daigt['model'] = 'unknown'  # Add model column for consistency\n",
    "\n",
    "df_daigt['text'] = df_daigt['text'].str.replace(r'[\\u2028\\u2029]', ' ', regex=True)\n",
    "print(f\"DAIGT Loaded: {len(df_daigt)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10663af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### joining csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc7f9632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after stacking: 20000\n",
      "Found 0 duplicate essays.\n",
      "Total Unique Rows: 20000\n",
      "Total Rows: 20000\n",
      "AI samples: 9017\n",
      "Human samples: 10983\n"
     ]
    }
   ],
   "source": [
    "df_master = pd.concat([df_raid, df_daigt], ignore_index=True)\n",
    "\n",
    "print(f\"Rows after stacking: {len(df_master)}\")\n",
    "\n",
    "duplicate_count = df_master.duplicated(subset=['text']).sum()\n",
    "print(f\"Found {duplicate_count} duplicate essays.\")\n",
    "\n",
    "df_master = df_master.drop_duplicates(subset=['text'])\n",
    "\n",
    "print(f\"Total Unique Rows: {len(df_master)}\")\n",
    "print(f\"Total Rows: {len(df_master)}\")\n",
    "print(f\"AI samples: {df_master['label'].value_counts()[1]}\")\n",
    "print(f\"Human samples: {df_master['label'].value_counts()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5071119",
   "metadata": {},
   "source": [
    "#### balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0dd8bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Count: 18034 rows (9,017 AI & 9,017 Human)\n",
      "Saved to: ../data/master_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "df_master = df_master.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "min_size = min(df_master['label'].value_counts())\n",
    "df_balanced = df_master.groupby('label').head(min_size)\n",
    "\n",
    "df_balanced.to_csv(\"../data/master_training_data.csv\", index=False)\n",
    "\n",
    "ai_count = len(df_balanced[df_balanced['label'] == 1])\n",
    "human_count = len(df_balanced[df_balanced['label'] == 0])\n",
    "\n",
    "print(f\"Final Count: {len(df_balanced)} rows ({ai_count:,} AI & {human_count:,} Human)\")\n",
    "print(\"Saved to: ../data/master_training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5f6e7",
   "metadata": {},
   "source": [
    "## Adding HC3 Dataset (Human vs ChatGPT Comparison)\n",
    "Source: https://huggingface.co/datasets/Hello-SimpleAI/HC3\n",
    "\n",
    "This dataset contains ~24,000 Q&A pairs with human answers vs ChatGPT answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "deacf3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading HC3 dataset from Hugging Face...\n",
      "HC3 Downloaded: 48644 Q&A pairs\n"
     ]
    }
   ],
   "source": [
    "# Download HC3 dataset (JSONL export)\n",
    "print(\"Downloading HC3 dataset from Hugging Face...\")\n",
    "\n",
    "from huggingface_hub import HfFileSystem\n",
    "\n",
    "fs = HfFileSystem()\n",
    "\n",
    "# Find JSONL files in the dataset repo\n",
    "all_files = fs.find(\"datasets/Hello-SimpleAI/HC3\")\n",
    "jsonl_files = [f for f in all_files if f.endswith(\".jsonl\")]\n",
    "\n",
    "if not jsonl_files:\n",
    "    print(\"No JSONL files found for HC3.\")\n",
    "    print(\"Top-level files:\")\n",
    "    print(fs.ls(\"datasets/Hello-SimpleAI/HC3\"))\n",
    "    raise ValueError(\"HC3 JSONL files not found. Check dataset structure or network.\")\n",
    "\n",
    "jsonl_files = [f\"hf://{path}\" for path in jsonl_files]\n",
    "\n",
    "# Load via JSONL (dataset scripts are deprecated)\n",
    "ds_hc3 = load_dataset(\"json\", data_files=jsonl_files, split=\"train\")\n",
    "print(f\"HC3 Downloaded: {len(ds_hc3)} Q&A pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45cf1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HC3 Converted: 170840 samples\n",
      "  Human: 117080\n",
      "  AI:    53760\n"
     ]
    }
   ],
   "source": [
    "# Convert HC3 to our training format\n",
    "hc3_samples = []\n",
    "\n",
    "for item in ds_hc3:\n",
    "    question = item['question']\n",
    "    \n",
    "    # Add human answers (label = 0)\n",
    "    for answer in item['human_answers']:\n",
    "        if answer and len(answer.strip()) > 20:\n",
    "            hc3_samples.append({\n",
    "                'text': f\"{question} {answer}\",\n",
    "                'label': 0,\n",
    "                'model': 'human'\n",
    "            })\n",
    "    \n",
    "    # Add ChatGPT answers (label = 1)\n",
    "    for answer in item['chatgpt_answers']:\n",
    "        if answer and len(answer.strip()) > 20:\n",
    "            hc3_samples.append({\n",
    "                'text': f\"{question} {answer}\",\n",
    "                'label': 1,\n",
    "                'model': 'chatgpt'\n",
    "            })\n",
    "\n",
    "df_hc3 = pd.DataFrame(hc3_samples)\n",
    "df_hc3['text'] = df_hc3['text'].str.replace(r'[\\u2028\\u2029]', ' ', regex=True)\n",
    "\n",
    "print(f\"HC3 Converted: {len(df_hc3)} samples\")\n",
    "print(f\"  Human: {len(df_hc3[df_hc3['label'] == 0])}\")\n",
    "print(f\"  AI:    {len(df_hc3[df_hc3['label'] == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c724f",
   "metadata": {},
   "source": [
    "## Adding GPT-wiki-intro Dataset (Wikipedia vs GPT)\n",
    "Source: https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro\n",
    "\n",
    "We will sample a limited number of rows to avoid over-weighting this genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41c997cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GPT-wiki-intro dataset from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bdf8ae054a411fb4185962596ca05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaina\\Documents\\ai_detector\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zaina\\.cache\\huggingface\\hub\\datasets--aadityaubhat--GPT-wiki-intro. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-wiki-intro loaded: 20000 rows\n",
      "GPT-wiki-intro Converted: 40000 samples\n",
      "  Human: 20000\n",
      "  AI:    20000\n"
     ]
    }
   ],
   "source": [
    "# Download GPT-wiki-intro dataset (sampled)\n",
    "print(\"Downloading GPT-wiki-intro dataset from Hugging Face...\")\n",
    "\n",
    "WIKI_SAMPLE_SIZE = 20000  # adjust if you want more/less\n",
    "\n",
    "# Streaming keeps memory low\n",
    "wiki_stream = load_dataset(\"aadityaubhat/GPT-wiki-intro\", split=\"train\", streaming=True)\n",
    "wiki_rows = list(wiki_stream.take(WIKI_SAMPLE_SIZE))\n",
    "\n",
    "print(f\"GPT-wiki-intro loaded: {len(wiki_rows)} rows\")\n",
    "\n",
    "# Build human vs AI rows\n",
    "wiki_samples = []\n",
    "for row in wiki_rows:\n",
    "    wiki_intro = row.get(\"wiki_intro\", \"\")\n",
    "    gen_intro = row.get(\"generated_intro\", \"\")\n",
    "    \n",
    "    if wiki_intro and len(wiki_intro.strip()) > 20:\n",
    "        wiki_samples.append({\n",
    "            \"text\": wiki_intro,\n",
    "            \"label\": 0,\n",
    "            \"model\": \"human\"\n",
    "        })\n",
    "    if gen_intro and len(gen_intro.strip()) > 20:\n",
    "        wiki_samples.append({\n",
    "            \"text\": gen_intro,\n",
    "            \"label\": 1,\n",
    "            \"model\": \"gpt\"\n",
    "        })\n",
    "\n",
    "df_wiki = pd.DataFrame(wiki_samples)\n",
    "df_wiki[\"text\"] = df_wiki[\"text\"].str.replace(r\"[\\u2028\\u2029]\", \" \", regex=True)\n",
    "\n",
    "print(f\"GPT-wiki-intro Converted: {len(df_wiki)} samples\")\n",
    "print(f\"  Human: {len(df_wiki[df_wiki['label'] == 0])}\")\n",
    "print(f\"  AI:    {len(df_wiki[df_wiki['label'] == 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc78e9",
   "metadata": {},
   "source": [
    "## Combining All Datasets\n",
    "Merge RAID + DAIGT + HC3 + GPT-wiki-intro into one master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02b8bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after stacking all datasets: 230840\n",
      "Found 88436 duplicate texts\n",
      "Total Unique Rows: 142404\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets\n",
    "df_combined = pd.concat([df_raid, df_daigt, df_hc3, df_wiki], ignore_index=True)\n",
    "\n",
    "print(f\"Rows after stacking all datasets: {len(df_combined)}\")\n",
    "\n",
    "# Remove duplicates\n",
    "duplicate_count = df_combined.duplicated(subset=['text']).sum()\n",
    "print(f\"Found {duplicate_count} duplicate texts\")\n",
    "\n",
    "df_combined = df_combined.drop_duplicates(subset=['text'])\n",
    "print(f\"Total Unique Rows: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f25b4",
   "metadata": {},
   "source": [
    "## Generating Short Text Samples\n",
    "Create shorter samples from long texts to improve detection on short answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6b249a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52356 texts with 200+ words\n",
      "Generated 1000 short samples (30-70 words each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate word counts\n",
    "df_combined['word_count'] = df_combined['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# Get long texts (200+ words)\n",
    "long_texts = df_combined[df_combined['word_count'] > 200]\n",
    "print(f\"Found {len(long_texts)} texts with 200+ words\")\n",
    "\n",
    "# Generate 1000 short samples from long texts\n",
    "np.random.seed(42)\n",
    "short_samples = []\n",
    "\n",
    "for _, row in long_texts.sample(min(1000, len(long_texts))).iterrows():\n",
    "    text = str(row['text'])\n",
    "    words = text.split()\n",
    "    \n",
    "    if len(words) > 100:\n",
    "        # Extract a chunk from the middle (30-70 words)\n",
    "        start = np.random.randint(10, len(words) - 60)\n",
    "        length = np.random.randint(30, 70)\n",
    "        short_text = ' '.join(words[start:start+length])\n",
    "        \n",
    "        short_samples.append({\n",
    "            'text': short_text,\n",
    "            'label': row['label'],\n",
    "            'model': row.get('model', 'unknown')\n",
    "        })\n",
    "\n",
    "df_short = pd.DataFrame(short_samples)\n",
    "print(f\"Generated {len(df_short)} short samples (30-70 words each)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa93994",
   "metadata": {},
   "source": [
    "## Final Balancing and Export\n",
    "Balance the enhanced dataset and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24fb7497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Enhanced Dataset: 111582 samples\n",
      "  Human: 55791\n",
      "  AI:    55791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaina\\AppData\\Local\\Temp\\ipykernel_5620\\854700945.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced_final = df_final.groupby('label').apply(\n"
     ]
    }
   ],
   "source": [
    "# Add short samples to combined dataset\n",
    "df_final = pd.concat([\n",
    "    df_combined.drop('word_count', axis=1), \n",
    "    df_short\n",
    "], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Balance the dataset\n",
    "min_size = min(df_final['label'].value_counts())\n",
    "df_balanced_final = df_final.groupby('label').apply(\n",
    "    lambda x: x.sample(min_size, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Shuffle again\n",
    "df_balanced_final = df_balanced_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final Enhanced Dataset: {len(df_balanced_final)} samples\")\n",
    "print(f\"  Human: {len(df_balanced_final[df_balanced_final['label'] == 0])}\")\n",
    "print(f\"  AI:    {len(df_balanced_final[df_balanced_final['label'] == 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "504445a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Length Distribution:\n",
      "  Min:    1 words\n",
      "  Max:    7934 words\n",
      "  Mean:   193 words\n",
      "  Median: 169 words\n",
      "\n",
      "  Short (<50 words):    4406\n",
      "  Medium (50-200):      65346\n",
      "  Long (200+):          41830\n"
     ]
    }
   ],
   "source": [
    "# Show text length distribution\n",
    "df_balanced_final['word_count'] = df_balanced_final['text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nText Length Distribution:\")\n",
    "print(f\"  Min:    {df_balanced_final['word_count'].min()} words\")\n",
    "print(f\"  Max:    {df_balanced_final['word_count'].max()} words\")\n",
    "print(f\"  Mean:   {df_balanced_final['word_count'].mean():.0f} words\")\n",
    "print(f\"  Median: {df_balanced_final['word_count'].median():.0f} words\")\n",
    "\n",
    "print(f\"\\n  Short (<50 words):    {len(df_balanced_final[df_balanced_final['word_count'] < 50])}\")\n",
    "print(f\"  Medium (50-200):      {len(df_balanced_final[(df_balanced_final['word_count'] >= 50) & (df_balanced_final['word_count'] < 200)])}\")\n",
    "print(f\"  Long (200+):          {len(df_balanced_final[df_balanced_final['word_count'] >= 200])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21d57793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/enhanced_training_data.csv\n",
      "Updated: ../data/master_training_data.csv\n",
      "\n",
      "==================================================\n",
      "DONE! Run 'python train.py' to retrain the model\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Save the enhanced dataset\n",
    "df_balanced_final.drop('word_count', axis=1).to_csv(\"../data/enhanced_training_data.csv\", index=False)\n",
    "print(\"Saved: ../data/enhanced_training_data.csv\")\n",
    "\n",
    "# Also update the master_training_data.csv\n",
    "df_balanced_final.drop('word_count', axis=1).to_csv(\"../data/master_training_data.csv\", index=False)\n",
    "print(\"Updated: ../data/master_training_data.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DONE! Run 'python train.py' to retrain the model\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314fbac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
